---
---
@article{larsen2022unmasking,
  abbr={arXiv},
  title={Lottery Tickets on a Data Diet: Finding Initializations with Sparse Trainable Networks},
  author={Paul*, Mansheej and Chen*, Feng and Larsen*, Brett W. and Frankle, Jonathan and Ganguli, Surya and Dziugaite, Gintare Karolina},
  journal={arXiv preprint arXiv:2210.03044,},
  year={2022},
  arxiv={2210.03044},
  abstract={Modern deep learning involves training costly, highly overparameterized networks, thus motivating the search for sparser networks that can still be trained to the same accuracy as the full network (i.e. matching). Iterative magnitude pruning (IMP) is a state of the art algorithm that can find such highly sparse matching subnetworks, known as winning tickets. IMP operates by iterative cycles of training, masking smallest magnitude weights, rewinding back to an early training point, and repeating. Despite its simplicity, the underlying principles for when and how IMP finds winning tickets remain elusive. In particular, what useful information does an IMP mask found at the end of training convey to a rewound network near the beginning of training? How does SGD allow the network to extract this information? And why is iterative pruning needed? We develop answers in terms of the geometry of the error landscape. First, we find that—at higher sparsities—pairs of pruned networks at successive pruning iterations are connected by a linear path with zero error barrier if and only if they are matching. This indicates that masks found at the end of training convey the identity of an axial subspace that intersects a desired linearly connected mode of a matching sublevel set. Second, we show SGD can exploit this information due to a strong form of robustness: it can return to this mode despite strong perturbations early in training. Third, we show how the flatness of the error landscape at the end of training determines a limit on the fraction of weights that can be pruned at each iteration of IMP. Finally, we show that the role of retraining in IMP is to find a network with new small weights to prune. Overall, these results make progress toward demystifying the existence of winning tickets by revealing the fundamental role of error landscape geometry.}
}

@article{larsen2022lotteryDiet,
  abbr={NeurIPS},
  title={Lottery Tickets on a Data Diet: Finding Initializations with Sparse Trainable Networks},
  author={Paul*, Mansheej and Larsen*, Brett W. and Ganguli, Surya and Frankle, Jonathan and Dziugaite, Gintare Karolina},
  journal={36th Conference on Neural Information Processing Systems,},
  year={2022},
  arxiv={2206.01278},
  code={https://github.com/mansheej/lth_diet},
  honor={Spotlight presentation at the Sparsity in Neural Networks (SNN) Workshop 2022.},
  abstract={A striking observation about iterative magnitude pruning (IMP; Frankle et al. 2020) is that — after just a few hundred steps of dense training — the method can find a sparse sub-network that can be trained to the same accuracy as the dense network. However, the same does not hold at step 0, i.e. random initialization. In this work, we seek to understand how this early phase of pre-training leads to a good initialization for IMP both through the lens of the data distribution and the loss landscape geometry. Empirically we observe that, holding the number of pre-training iterations constant, training on a small fraction of (randomly chosen) data suffices to obtain an equally good initialization for IMP. We additionally observe that by pre-training only on "easy" training data, we can decrease the number of steps necessary to find a good initialization for IMP compared to training on the full dataset or a randomly chosen subset. Finally, we identify novel properties of the loss landscape of dense networks that are predictive of IMP performance, showing in particular that more examples being linearly mode connected in the dense network correlates well with good initializations for IMP. Combined, these results provide new insight into the role played by the early phase training in IMP.}
}

@article{larsen2020practical,
  abbr={SIMAX},
  title={Practical leverage-based sampling for low-rank tensor decomposition},
  author={Larsen, Brett W. and Kolda, Tamara G.},
  journal={SIAM Journal on Matrix Analysis and Applications (SIMAX),},
  year={2022},
  doi={https://doi.org/10.1137/21M1441754},
  arxiv={2006.16438},
  code={https://github.com/bwlarsen/cp-arls-lev-experiments},
  abstract={  The low-rank canonical polyadic tensor decomposition is useful in data analysis and can be computed by solving a sequence of overdetermined least squares subproblems. Motivated by consideration of sparse tensors, we propose sketching each subproblem using leverage scores to select a subset of the rows, with probabilistic guarantees on the solution accuracy. We randomly sample rows proportional to leverage score upper bounds that can be efficiently computed using the special Khatri-Rao subproblem structure inherent in tensor decomposition. Crucially,  for a \((d+1)\)-way tensor, the number of rows in the sketched system is \(O(r^d/\epsilon)\) for a decomposition of rank \(r\) and \(\epsilon\)-accuracy in the least squares solve, independent of both the size and the number of nonzeros in the tensor. Along the way, we provide a practical solution to the generic matrix sketching problem of sampling overabundance for high-leverage-score rows, proposing to include such rows deterministically and combine repeated samples in the sketched system; we conjecture that this can lead to improved theoretical bounds. Numerical results on real-world large-scale tensors show the method is significantly faster than deterministic methods at nearly the same level of accuracy.}
}


@article{larsen2022towards,
  abbr={PLOS Comp Bio},
  title={Towards a more general understanding of the algorithmic utility of recurrent connections},
  author={Larsen, Brett W. and Druckmann, Shaul},
  journal={PLOS Computational Biology,},
  volume={18},
  number={6},
  pages={e1010227},
  year={2022},
  publisher={Public Library of Science San Francisco, CA USA},
  doi = {https://doi.org/10.1371/journal.pcbi.1010227},
  code = {https://github.com/druckmann-lab/edgeConnectedPixel},
  honor={Poster presentation at COSYNE 2019.},
  abstract={Lateral and recurrent connections are ubiquitous in biological neural circuits. Yet while the strong computational abilities of feedforward networks have been extensively studied, our understanding of the role and advantages of recurrent computations that might explain their prevalence remains an important open challenge. Foundational studies by Minsky and Roelfsema argued that computations that require propagation of global information for local computation to take place would particularly benefit from the sequential, parallel nature of processing in recurrent networks. Such “tag propagation” algorithms perform repeated, local propagation of information and were originally introduced in the context of detecting connectedness, a task that is challenging for feedforward networks. Here, we advance the understanding of the utility of lateral and recurrent computation by first performing a large-scale empirical study of neural architectures for the computation of connectedness to explore feedforward solutions more fully and establish robustly the importance of recurrent architectures. In addition, we highlight a tradeoff between computation time and performance and construct hybrid feedforward/recurrent models that perform well even in the presence of varying computational time limitations. We then generalize tag propagation architectures to propagating multiple interacting tags and demonstrate that these are efficient computational substrates for more general computations of connectedness by introducing and solving an abstracted biologically inspired decision-making task. Our work thus clarifies and expands the set of computational tasks that can be solved efficiently by recurrent computation, yielding hypotheses for structure in population activity that may be present in such tasks.}
}

@article{larsen2022many,
  abbr={ICLR},
  title={How many degrees of freedom do we need to train deep networks: a loss landscape perspective},
  author={Larsen, Brett W. and Fort, Stanislav and Becker, Nic and Ganguli, Surya},
  journal={International Conference on Learning Representations,},
  year={2022},
  pdf={https://openreview.net/forum?id=ChMLTGRjFcU},
  arxiv={2107.05802},
  code={https://github.com/ganguli-lab/degrees-of-freedom},
  abstract={A variety of recent works, spanning pruning, lottery tickets, and training within random subspaces, have shown that deep neural networks can be trained using far fewer degrees of freedom than the total number of parameters. We analyze this phenomenon for random subspaces by first examining the success probability of hitting a training loss sublevel set when training within a random subspace of a given training dimensionality.  We find a sharp phase transition in the success probability from 0 to 1 as the training dimension surpasses a threshold. This threshold training dimension increases as the desired final loss decreases, but decreases as the initial loss decreases. We then theoretically explain the origin of this phase transition, and its dependence on initialization and final desired loss, in terms of properties of the high-dimensional geometry of the loss landscape.  In particular, we show via Gordon's escape theorem, that the training dimension plus the Gaussian width of the desired loss sublevel set, projected onto a unit sphere surrounding the initialization, must exceed the total number of parameters for the success probability to be large.  In several architectures and datasets, we measure the threshold training dimension as a function of initialization and demonstrate that it is a small fraction of the total parameters, implying by our theory that successful training with so few dimensions is possible precisely because the Gaussian width of low loss sublevel sets is very large. Moreover, we compare this threshold training dimension to more sophisticated ways of reducing training degrees of freedom, including lottery tickets as well as a new, analogous method: lottery subspaces.}
}




@article{kazemipour2020avoiding,
  abbr={arXiv},
  title={Avoiding Spurious Local Minima in Deep Quadratic Networks},
  author={Kazemipour, Abbas and Larsen, Brett W. and Druckmann, Shaul},
  journal={arXiv preprint arXiv:2001.00098,},
  year={2020},
  arxiv={2001.00098},
  honor={Oral presentation at Deep Math 2019.}
}

@mastersthesis{larsen2016graph,
  abbr={Thesis},
  author = {Brett W. Larsen},
  title = {Graph-Based Clustering: Distributed Algorithms for Balanced Graph Cuts},
  publisher = {University of Cambridge},
  year = {2016}
}

@inproceedings{larsen2013applying,
  abbr={SPIE},
  title={Applying matching pursuit decomposition time-frequency processing to UGS footstep classification},
  author={Larsen, Brett W. and Chung, Hugh and Dominguez, Alfonso and Sciacca, Jacob and Kovvali, Narayan and Papandreou-Suppappola, Antonia and Allee, David R},
  booktitle={Sensors, and Command, Control, Communications, and Intelligence (C3I) Technologies for Homeland Security and Homeland Defense XII},
  volume={8711},
  pages={871104},
  year={2013},
  organization={International Society for Optics and Photonics},
  doi={https://doi.org/10.1117/12.2015498}
}


@string{aps = {American Physical Society,}}

@book{einstein1956investigations,
  bibtex_show={true},
  title={Investigations on the Theory of the Brownian Movement},
  author={Einstein, Albert},
  year={1956},
  publisher={Courier Corporation,},
  preview={brownian-motion.gif}
}

@article{einstein1950meaning,
  abbr={AJP},
  bibtex_show={true},
  title={The meaning of relativity},
  author={Einstein, Albert and Taub, AH},
  journal={American Journal of Physics,},
  volume={18},
  number={6},
  pages={403--404},
  year={1950},
  publisher={American Association of Physics Teachers,}
}

@article{PhysRev.47.777,
  abbr={PhysRev},
  title={Can Quantum-Mechanical Description of Physical Reality Be Considered Complete?},
  author={Einstein, A. and Podolsky, B. and Rosen, N.},
  abstract={In a complete theory there is an element corresponding to each element of reality. A sufficient condition for the reality of a physical quantity is the possibility of predicting it with certainty, without disturbing the system. In quantum mechanics in the case of two physical quantities described by non-commuting operators, the knowledge of one precludes the knowledge of the other. Then either (1) the description of reality given by the wave function in quantum mechanics is not complete or (2) these two quantities cannot have simultaneous reality. Consideration of the problem of making predictions concerning a system on the basis of measurements made on another system that had previously interacted with it leads to the result that if (1) is false then (2) is also false. One is thus led to conclude that the description of reality as given by a wave function is not complete.},
  journal={Phys. Rev.,},
  volume={47},
  issue={10},
  pages={777--780},
  numpages={0},
  year={1935},
  month={May},
  publisher=aps,
  doi={10.1103/PhysRev.47.777},
  url={http://link.aps.org/doi/10.1103/PhysRev.47.777},
  html={https://journals.aps.org/pr/abstract/10.1103/PhysRev.47.777},
  pdf={example_pdf.pdf},
  selected={true}
}

@article{einstein1905molekularkinetischen,
  title={{\"U}ber die von der molekularkinetischen Theorie der W{\"a}rme geforderte Bewegung von in ruhenden Fl{\"u}ssigkeiten suspendierten Teilchen},
  author={Einstein, A.},
  journal={Annalen der physik,},
  volume={322},
  number={8},
  pages={549--560},
  year={1905},
  publisher={Wiley Online Library}
}

@article{einstein1905movement,
  abbr={Ann. Phys.},
  title={Un the movement of small particles suspended in statiunary liquids required by the molecular-kinetic theory 0f heat},
  author={Einstein, A.},
  journal={Ann. Phys.,},
  volume={17},
  pages={549--560},
  year={1905}
}

@article{einstein1905electrodynamics,
  title={On the electrodynamics of moving bodies},
  author={Einstein, A.},
  year={1905}
}

@book{przibram1967letters,
  bibtex_show={true},
  title={Letters on wave mechanics},
  author={Einstein, Albert and Schrödinger, Erwin and Planck, Max and Lorentz, Hendrik Antoon and Przibram, Karl},
  year={1967},
  publisher={Vision},
  preview={wave-mechanics.gif}
}
